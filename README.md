# TASK

## Question 1. Define Sample Probability and Real Probability

**Answer:**  

**Real Probability** ‚Äî The theoretical probability calculated using the number of favorable outcomes over the total possible outcomes.  

- **Formula:**  
  P(A) = Number of favourable outcomes / Total number of possible outcomes  
- **Example:**  
  When tossing a fair coin:  
  P(H) = 1/2 = 0.5  
- **Notes:**  
  - This probability **does not change**, no matter how many times you toss the coin (100, 200, etc.).  
  - It is based on the **model of fairness**.  

---

**Sample Probability** ‚Äî The probability calculated from **actual observations** or experimental data.  

- **Formula:**  
  P‚Çô(A) = n(A) / N  
  Where:  
  - N = Total number of trials  
  - n(A) = Number of times event A actually occurred  
- **Example:**  
  Tossing a coin 100 times and getting 76 heads:  
  P‚ÇÅ‚ÇÄ‚ÇÄ(H) = 76 / 100 = 0.76  
- **Notes:**  
  - This probability is based on **actual observation** from the sample.  

Hence, **sample probability** is our experimental estimate, while **real probability** represents the ideal theoretical model.

---

## Question 2. Difference Between Expectation and Average

Expectation and Average (Mean) both describe the central tendency of data, but they differ in how they account for the **probability of each value**.

---

### üîπ Average (Mean)
The **average** is a special case of the expected value where **all outcomes are equally likely**.

**Formula:**
```
XÃÑ = Œº = (Œ£X) / n
```

Where:  
- XÃÑ ‚Üí Sample mean  
- Œº ‚Üí Population mean  
- Œ£X ‚Üí Sum of all values  
- n ‚Üí Number of values  

**Example (Equal Probabilities):**  
If X = {1, 2, 3, 4, 5, 6}  
```
XÃÑ = (1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.5
```

---

### üîπ Expected Value
The **expected value** considers the **probability** of each outcome.

**Formula:**
```
E[X] = Œ£ (X * P(X))
```

Where:  
- X ‚Üí Each possible outcome  
- P(X) ‚Üí Probability of that outcome  

**Example 1 ‚Äî Equal Probabilities:**  
If X = {1, 2, 3, 4, 5, 6}  
and  
P(1)=1/6, P(2)=1/6, P(3)=1/6, P(4)=1/6, P(5)=1/6, P(6)=1/6  
Then,  
```
E[X] = 1*(1/6) + 2*(1/6) + 3*(1/6) + 4*(1/6) + 5*(1/6) + 6*(1/6)
E[X] = 3.5
```
Here,  
```
E[X] = XÃÑ
```
because all probabilities are equal.

**Example 2 ‚Äî Unequal Probabilities:**  
If X = {1, 2, 3, 4, 5, 6}  
and  
P(1)=3/20, P(2)=3/20, P(3)=3/20, P(4)=3/20, P(5)=1/5, P(6)=1/5  
Then,  
```
E[X] = 1*(3/20) + 2*(3/20) + 3*(3/20) + 4*(3/20) + 5*(1/5) + 6*(1/5)
E[X] = 3.7
```

Here,  
```
E[X] = 3.7
XÃÑ = 3.5
```

Thus, **average** is a special case of **expected value** when all probabilities are equal.

---

## Question 3. Law of the Unconscious Statistician (LOTUS)

### Definition
**LOTUS** stands for **Law of the Unconscious Statistician**.  
It allows us to compute the **expected value of a function of a random variable** without first finding the distribution of that function.

---

### üîπ Formula

#### For Discrete Random Variables:
```
E[g(X)] = Œ£ g(x) * P(X = x)
```

#### For Continuous Random Variables:
```
E[g(X)] = ‚à´ g(x) * f_X(x) dx
```

Where:  
- g(X) ‚Üí Function of the random variable X  
- P(X = x) ‚Üí Probability mass function (PMF)  
- f_X(x) ‚Üí Probability density function (PDF)

---

### üîπ Intuitive Explanation
To find **E[g(X)]**, you **don‚Äôt need to find the distribution of g(X)**.  
You can compute it directly using the **distribution of X**.

---

###  Example (Discrete Case)
If X = {1, 2, 3} with probabilities {0.2, 0.5, 0.3}, and we want E[X¬≤]:  
```
E[X¬≤] = Œ£ x¬≤ * P(X = x)
       = 1¬≤(0.2) + 2¬≤(0.5) + 3¬≤(0.3)
       = 0.2 + 2 + 2.7 = 4.9
```

---

###  Example (Continuous Case)
If X has PDF f(x) = 2x for 0 < x < 1, and we want E[X¬≤]:  
```
E[X¬≤] = ‚à´‚ÇÄ¬π x¬≤ * 2x dx
       = 2 ‚à´‚ÇÄ¬π x¬≥ dx
       = 2 * (1/4) = 0.5
```

---

## Question 5. How Generative Model uses Joint Probability Distribution for generating new data

**Generative Models** are a type of machine learning model that learn how data is generated, so they can create new data. For example, GPT models such as ChatGPT, Claude, and Grok are trained on large amounts of data, which helps them learn patterns from past data and generate new, realistic outputs.

**Joint Probability Distribution**  
For a set of variables X1, X2, ..., Xn, the joint probability distribution 

$$
P(X_1, X_2, ..., X_n)
$$

gives the probability of all these variables occurring together.

A generative model tries to learn the joint probability distribution \(P(X_1, X_2, ..., X_n)\) of the data so that it is close to the **true probability** \(P_\text{real}(X)\), which represents how the real data is naturally structured.  

The model's probability \(P_\theta(X)\) is the distribution predicted by the model, parameterized by Œ∏. It represents what the model thinks is likely based on the training data it has seen. The goal during training is to adjust Œ∏ so that \(P_\theta(X)\) gets as close as possible to \(P_\text{real}(X)\).  

By learning this joint probability distribution, the model can generate new data that follows the same patterns as real data.


---

## Question 9 . Consider a quiz game where a person is given two questions and must decide which question to answer first. Question 1 will be answered correctly with probability 0.8, and the person will then receive as prize $100, while question 2 will be answered correctly with probability 0.5, and the person will then receive as prize $200. If the first question attempted is answered incorrectly, the quiz terminates (the person is not allowed to attempt the second question). If the first question is answered correctly, the person is allowed to attempt the second question.Which question should be answered first to maximize the expected value of the total prize money received?
## ANSWER : The player should answer the 0.8-probability $100 question before the 0.5-probability $200 question.

Given in Question :-
| Question | Probability of Correct | Prize |
|----------|----------------------|-------|
| Q1       | 0.8                  | $100  |
| Q2       | 0.5                  | $200  |

It is clearly mentioned in question :
1. If the **first question is wrong**, the game ends.  
2. If the **first question is correct**, the player can attempt the second question.  

Here , we need to maximize expected total prize.
So, There are two cases 
#### Case 1:-Answer Q1 first, then Q2
1.Probability Q1 is correct = 0.8 ‚Üí Win $100
2.Probability Q2 is correct if Q1 was correct = 0.5 ‚Üí Win additional $200

Expected Value :
```
EV=(Probability¬†Q1¬†correct)√ó[Prize¬†for¬†Q1+(Probability¬†Q2¬†correct√óPrize¬†for¬†Q2)]
ùê∏ùëâ=0.8√ó[100+(0.5√ó200)]

ùê∏ùëâ=0.8√ó[100+100]=0.8√ó200
EV = $160
```

#### Case 2: Answer Q2 first, then Q1

Expected Value :
```
EV=(Probability¬†Q2¬†correct)√ó[Prize¬†for¬†Q2+(Probability¬†Q1¬†correct√óPrize¬†for¬†Q1)]
ùê∏ùëâ=0.5√ó[200+(0.8√ó100)]

ùê∏ùëâ=0.8√ó[200+18]=0.5√ó280
EV = $140
```
So even though Q2 offers a higher individual prize ($200), the higher probability of Q1 ($100 with 0.8 probability) gives a higher expected value when answered first.

1. If we start with Q2(0.5 probability),there's a 50 % chance that game ends immediately , so we never get the easier amount of $100 from Q1.
2. If we start with Q1 (0.8 probability),there's a high chance secure the smallerr prize first, and then we have still a chance to win a bigger prize.
