# TASK

## Question 1. Define Sample Probability and Real Probability

**Answer:**  

**Real Probability** ‚Äî The theoretical probability calculated using the number of favorable outcomes over the total possible outcomes.  

- **Formula:**  
  P(A) = Number of favourable outcomes / Total number of possible outcomes  
- **Example:**  
  When tossing a fair coin:  
  P(H) = 1/2 = 0.5  
- **Notes:**  
  - This probability **does not change**, no matter how many times you toss the coin (100, 200, etc.).  
  - It is based on the **model of fairness**.  

---

**Sample Probability** ‚Äî The probability calculated from **actual observations** or experimental data.  

- **Formula:**  
  P‚Çô(A) = n(A) / N  
  Where:  
  - N = Total number of trials  
  - n(A) = Number of times event A actually occurred  
- **Example:**  
  Tossing a coin 100 times and getting 76 heads:  
  P‚ÇÅ‚ÇÄ‚ÇÄ(H) = 76 / 100 = 0.76  
- **Notes:**  
  - This probability is based on **actual observation** from the sample.  

Hence, **sample probability** is our experimental estimate, while **real probability** represents the ideal theoretical model.

---

## Question 2. Difference Between Expectation and Average

Expectation and Average (Mean) both describe the central tendency of data, but they differ in how they account for the **probability of each value**.

---

### üîπ Average (Mean)
The **average** is a special case of the expected value where **all outcomes are equally likely**.

**Formula:**
```
XÃÑ = Œº = (Œ£X) / n
```

Where:  
- XÃÑ ‚Üí Sample mean  
- Œº ‚Üí Population mean  
- Œ£X ‚Üí Sum of all values  
- n ‚Üí Number of values  

**Example (Equal Probabilities):**  
If X = {1, 2, 3, 4, 5, 6}  
```
XÃÑ = (1 + 2 + 3 + 4 + 5 + 6) / 6 = 3.5
```

---

### üîπ Expected Value
The **expected value** considers the **probability** of each outcome.

**Formula:**
```
E[X] = Œ£ (X * P(X))
```

Where:  
- X ‚Üí Each possible outcome  
- P(X) ‚Üí Probability of that outcome  

**Example 1 ‚Äî Equal Probabilities:**  
If X = {1, 2, 3, 4, 5, 6}  
and  
P(1)=1/6, P(2)=1/6, P(3)=1/6, P(4)=1/6, P(5)=1/6, P(6)=1/6  
Then,  
```
E[X] = 1*(1/6) + 2*(1/6) + 3*(1/6) + 4*(1/6) + 5*(1/6) + 6*(1/6)
E[X] = 3.5
```
Here,  
```
E[X] = XÃÑ
```
because all probabilities are equal.

**Example 2 ‚Äî Unequal Probabilities:**  
If X = {1, 2, 3, 4, 5, 6}  
and  
P(1)=3/20, P(2)=3/20, P(3)=3/20, P(4)=3/20, P(5)=1/5, P(6)=1/5  
Then,  
```
E[X] = 1*(3/20) + 2*(3/20) + 3*(3/20) + 4*(3/20) + 5*(1/5) + 6*(1/5)
E[X] = 3.7
```

Here,  
```
E[X] = 3.7
XÃÑ = 3.5
```

Thus, **average** is a special case of **expected value** when all probabilities are equal.

---

## Question 3. Law of the Unconscious Statistician (LOTUS)

### Definition
**LOTUS** stands for **Law of the Unconscious Statistician**.  
It allows us to compute the **expected value of a function of a random variable** without first finding the distribution of that function.

---

### üîπ Formula

#### For Discrete Random Variables:
```
E[g(X)] = Œ£ g(x) * P(X = x)
```

#### For Continuous Random Variables:
```
E[g(X)] = ‚à´ g(x) * f_X(x) dx
```

Where:  
- g(X) ‚Üí Function of the random variable X  
- P(X = x) ‚Üí Probability mass function (PMF)  
- f_X(x) ‚Üí Probability density function (PDF)

---

### üîπ Intuitive Explanation
To find **E[g(X)]**, you **don‚Äôt need to find the distribution of g(X)**.  
You can compute it directly using the **distribution of X**.

---

###  Example (Discrete Case)
If X = {1, 2, 3} with probabilities {0.2, 0.5, 0.3}, and we want E[X¬≤]:  
```
E[X¬≤] = Œ£ x¬≤ * P(X = x)
       = 1¬≤(0.2) + 2¬≤(0.5) + 3¬≤(0.3)
       = 0.2 + 2 + 2.7 = 4.9
```

---

###  Example (Continuous Case)
If X has PDF f(x) = 2x for 0 < x < 1, and we want E[X¬≤]:  
```
E[X¬≤] = ‚à´‚ÇÄ¬π x¬≤ * 2x dx
       = 2 ‚à´‚ÇÄ¬π x¬≥ dx
       = 2 * (1/4) = 0.5
```

